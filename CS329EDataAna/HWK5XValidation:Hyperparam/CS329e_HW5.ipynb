{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "CS329e_HW5(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfuIVwk6VQNg"
      },
      "source": [
        "## C S 329E HW 5\n",
        "\n",
        "# Crossvalidation and hyperparameter selection\n",
        "\n",
        "## Jeremy Ulfohn // A5 Group 3 // jau392\n",
        "## Anthony Roth-Giacinto // A5 Group 3 // ajr4498\n",
        "\n",
        "For this week's homework we are going to explore the cross validation testing methodology and applying that to get error estimates on the two algorithms we've used so far:\n",
        "  - Linear Regression\n",
        "  - Decision Tree classification\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96E_96hMVRiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f870fdd7-2fd3-43a3-f1cf-4440b4bd8a40"
      },
      "source": [
        "# Google colab's default version of scikit-learn isn't the latest, so you will \n",
        "# need to update the virtual machine and restart the runtime\n",
        "!pip install scikit-learn==1.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.0\n",
            "  Downloading scikit_learn-1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.1 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0) (1.4.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-1.0 threadpoolctl-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDHKJDFmVQNi"
      },
      "source": [
        "# import the libraries! If you want to add things here for visualization, please do, \n",
        "# but only use sklearn when prompted\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import tree \n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_diabetes"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZs2nuNbVQNj"
      },
      "source": [
        "# Part 1 - Calculate Generalized Error on Linear Regression with k-fold Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JVk1iaWVQNk"
      },
      "source": [
        "## Q1.1 Load in the diabetes data set as a pandas dataframe and series.  \n",
        "Documentation on the data set is [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html).  Name your features dataframe (the independent variables) `df_X` and your target value (the dependent variable) series `s_y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQVbOpO9VQNk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6a816b75-61d7-47b1-d829-d122c62bbaac"
      },
      "source": [
        "tup = load_diabetes(return_X_y=True, as_frame=True) # X_y returns a tuple, then as_frame converts each element into a Pandas df\n",
        "df_X = tup[0]\n",
        "s_y = tup[1]\n",
        "df_X.head()\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age       sex       bmi  ...        s4        s5        s6\n",
              "0  0.038076  0.050680  0.061696  ... -0.002592  0.019908 -0.017646\n",
              "1 -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068330 -0.092204\n",
              "2  0.085299  0.050680  0.044451  ... -0.002592  0.002864 -0.025930\n",
              "3 -0.089063 -0.044642 -0.011595  ...  0.034309  0.022692 -0.009362\n",
              "4  0.005383 -0.044642 -0.036385  ... -0.002592 -0.031991 -0.046641\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzgW4G5SVQNk"
      },
      "source": [
        "## Q1.2 Define a function that creates a linear least squares regression model and a function to predict a continuous value given a linear regression model\n",
        "The first function should take in two parameters, `df_X`, and `s_y` and return the least squares regression model, $\\hat{\\beta}$ (using the notation from the ESLII book chapter 3 and HW3).  You can not use any libraries outside of pandas and numpy. Note that the length of beta_hat should be the number of columns in `df_X` + 1. \n",
        "\n",
        "The second function should take in two parameters, `beta_hat` - the model generated from the `get_linear_regression` function, and `df_X` - that has the attribute values at which we want to predict a continuous value.  We assume that the format and ordering of `df_X` used to create the model and `df_X` used to generate predictions are consistent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf37fNLUVQNl"
      },
      "source": [
        "def get_linear_regression_model( df_X, s_y ): # NOTE: df_X length = 10\n",
        "    # first column is the market\n",
        "    X = df_X # readability\n",
        "    y = s_y\n",
        "    # prepend a column of ones for the intercept\n",
        "    X = np.c_[np.ones(X.shape[0]), X] # .c_ is concatenation on 2nd axis\n",
        "    # ... whereas .r_ (Rows) is that along 1st axis (rows)\n",
        "\n",
        "    # matrix algebra to find betaHat\n",
        "    b = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "    return b\n",
        "# confirm correct amount of rows === 11: DONE"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaM8sfLCVQNm"
      },
      "source": [
        "# code to check beta_hat\n",
        "# FIXME: use assert_equal()\n",
        "np.random.seed(23) \n",
        "beta_hat = get_linear_regression_model( pd.DataFrame(np.random.random((34,4))), pd.Series(np.random.random(34)*10.0) )\n",
        "# length of beta_hat == 5\n",
        "testdf = pd.DataFrame(np.random.random((3,4))) # random df is 3 rows X 4 col\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPpM4U6YVQNm"
      },
      "source": [
        "def predict_linear_regression_value( beta_hat, df_X ):\n",
        "  # prepend column of 1's to given df_X\n",
        "  # REMEMBER THIS! Because this matrix method must handle the intercept, hence the prepension\n",
        "  mod_df_X = np.c_[np.ones(df_X.shape[0]), df_X] # .c_[] a col of 1's to df_X\n",
        "\n",
        "  return np.matmul(mod_df_X, beta_hat)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyMGfxtxXNCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7000b968-f54e-47a2-8c99-a4ada000b3cb"
      },
      "source": [
        "predicted_vals = predict_linear_regression_value(beta_hat, testdf)\n",
        "predicted_vals\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.03669521, 4.39502589, 3.90884703])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRxs-OTiVQNn"
      },
      "source": [
        "## Q1.3 Define a function that partitions the dataframe and series data into dictionaries\n",
        "This function should take in three parameters, `df_X`, `s_y`, and `k`, and returns a tuple of two dictionaries.\n",
        "In both dictionaries the key is the `k` value (an integer from one to `k` inclusive).\n",
        "The first dictionary will have the dataframe of the training data that contains approximately $\\frac{1}{k}$ of the data (variation due to randomness is acceptable).\n",
        "The second dictionary will have the series of the target data that contains approximately $\\frac{1}{k}$ of the data (variation due to randomness is acceptable). Please note the targets _must match_ the same rows as the dataframe at this index, e.g, the length of the kth partition is the same for the dataframe and series.\n",
        "\n",
        "Call that function with $k=5$ and create the dictionaries `dict_k_df_X` and `dict_k_s_y`. Print out the number of rows in each fold.  Check that the number of data points in each partition totals the number of data points in the entire dataset. \n",
        "\n",
        "Here is some example output from checking the length of the folds:\n",
        "```\n",
        "Fold 1 length of dataframe is 88 and length of series is 88\n",
        "Fold 2 length of dataframe is 96 and length of series is 96\n",
        "Fold 3 length of dataframe is 88 and length of series is 88\n",
        "Fold 4 length of dataframe is 79 and length of series is 79\n",
        "Fold 5 length of dataframe is 91 and length of series is 91\n",
        "The sum of the number of elements in each fold is 442 and there are 442 rows in the original df\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKZFErNNVQNo"
      },
      "source": [
        "# k := key, type int\n",
        "# output: tuple (dictX, dicty), with each containing 1/k of data\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def get_part_indices(df_X, k):\n",
        "  kf = KFold(n_splits=k)\n",
        "  folds = kf.split(df_X)\n",
        "  index_list = []\n",
        "  for _ in range(k):\n",
        "    train_index, test_index = next(folds)\n",
        "    index_list.append(test_index)\n",
        "  return index_list\n",
        "\n",
        "def partition_data(df_X, s_y, k): # yields a list of tuples (dict_X, dict_y) of length k\n",
        "  dict_X = {} # initialize dicts\n",
        "  dict_y = {}\n",
        "\n",
        "  keyval = 1 # int from 1 to k\n",
        "  for test_index in get_part_indices(df_X, k):\n",
        "    # create the train(X) and target(y) partitions\n",
        "    partition_X = df_X.iloc[test_index,:]\n",
        "    partition_y = s_y.iloc[test_index]\n",
        "\n",
        "    # update dictionary\n",
        "    dict_X.update({keyval: partition_X})\n",
        "    dict_y.update({keyval: partition_y})\n",
        "    keyval+=1 # increment keyval\n",
        "\n",
        "  return (dict_X, dict_y) # tuple containing dictionaries containing dfs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pefH5wfJVQNo"
      },
      "source": [
        "# define list 'all', containing all of the k partitions\n",
        "(dict_k_df_X, dict_k_s_y) = partition_data(df_X, s_y, 5)\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ27tMRFVQNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92811a6f-8019-44f3-f1e7-857f4bf32673"
      },
      "source": [
        "# Check fold sizes\n",
        "# print out number of rows in each fold\n",
        "\n",
        "print(\"Fold 1 length of dataframe is \", len(dict_k_df_X[1].index),\\\n",
        "      \" and length of series is \", len(dict_k_s_y[1].index)  )\n",
        "print(\"Fold 2 length of dataframe is \", len(dict_k_df_X[2].index),\\\n",
        "      \" and length of series is \", len(dict_k_s_y[2].index)  )\n",
        "print(\"Fold 3 length of dataframe is \", len(dict_k_df_X[3].index),\\\n",
        "      \" and length of series is \", len(dict_k_s_y[3].index)  )\n",
        "print(\"Fold 4 length of dataframe is \", len(dict_k_df_X[4].index),\\\n",
        "      \" and length of series is \", len(dict_k_s_y[4].index)  )\n",
        "print(\"Fold 5 length of dataframe is \", len(dict_k_df_X[5].index),\\\n",
        "      \" and length of series is \", len(dict_k_s_y[5].index)  )\n",
        "print(\"Sum of aforementioned lengths = 442\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 length of dataframe is  89  and length of series is  89\n",
            "Fold 2 length of dataframe is  89  and length of series is  89\n",
            "Fold 3 length of dataframe is  88  and length of series is  88\n",
            "Fold 4 length of dataframe is  88  and length of series is  88\n",
            "Fold 5 length of dataframe is  88  and length of series is  88\n",
            "Sum of aforementioned lengths = 442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtdijUdNVQNp"
      },
      "source": [
        "## Q1.4 Define a function that calculates a regression metric\n",
        "This function should accept two series of equal length $n$ numpy arrays, `s_y`, and `s_y_hat`. The metric it should calculate is the mean absolute error, $MAE = \\sum\\limits_{i=1}^n\\frac{|{s\\_y_i - {s\\_y\\_hat}_i}|}{n}$ \n",
        "\n",
        "Test your function by using the vectors:\n",
        "```\n",
        "x = np.array([1,2,3])\n",
        "y = np.array([2,2,3])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v40R9HM_VQNq"
      },
      "source": [
        "def get_mae( s_y, s_y_hat): # assume equal length\n",
        "  mae = 0\n",
        "  n = len(s_y)\n",
        "  for ind in range(n):\n",
        "    mae += abs(s_y[ind] - s_y_hat[ind])\n",
        "  return mae/n\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJB-KKKvVQNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58549cfe-a756-4c98-bcd5-c8f566d16746"
      },
      "source": [
        "# Test it \n",
        "x = np.array([1,2,3])\n",
        "y = np.array([2,2,3]) # :== s_y_hat\n",
        "get_mae(x,y)\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pgVtzgWVQNq"
      },
      "source": [
        "## Q1.5 Calculate the $MAE$ for each fold\n",
        "For each fold in your dictionaries, calculate the $MAE$.  Use the partition number key as the test set, and all other partitions as the train set. \n",
        "\n",
        "Print the min, max, and mean $MAE$ of your 5 folds. \n",
        "\n",
        "You must use your helper functions that you wrote above, `get_linear_regression_model`, `predict_linear_regression_value` and `get_mae`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuxFj8z5VQNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65afd39-e053-45c7-f3d8-c3c45e16fa30"
      },
      "source": [
        "mae = np.array([])\n",
        "r = [1, 2, 3, 4, 5]\n",
        "for k in dict_k_df_X.keys(): # i.e. in [1, 2, 3, 4, 5]\n",
        "  r.pop(0)\n",
        "  # first: 1 TEST, 2 TRAIN\n",
        "  test = dict_k_s_y[k] # series\n",
        "  train = dict_k_df_X[k]\n",
        "\n",
        "  # use helper functions to determine s_y_hat for this k\n",
        "  betaHat = get_linear_regression_model(train, test)\n",
        "  values = predict_linear_regression_value(betaHat, train) # np.array type\n",
        "  \n",
        "  # need both get_mae parameters to be type np.array\n",
        "  s_y = np.array(test)\n",
        "  s_y_hat = np.array(values)\n",
        "\n",
        "  # calculate M.A.E. for this k value\n",
        "  mae = np.append(mae, get_mae(s_y, s_y_hat))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([38.59551547, 41.77984083, 45.14311716, 39.83699235, 42.09152503])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OxejniBVQNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9679d73a-cb74-4b1a-e1a8-7071f7e237f7"
      },
      "source": [
        "print(\"The min MAE is {:.2f}, the max MAE is {:.2f}, and the mean MAE is {:.2f}\".format(mae.min(),mae.max(),mae.mean()))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The min MAE is 38.60, the max MAE is 45.14, and the mean MAE is 41.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqOqK30FVQNr"
      },
      "source": [
        "# Part 2 - Find the best hyperparameter to use in a Decision Tree "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xVkJfkdVQNr"
      },
      "source": [
        "## Q2.1 Load the iris data in as a pandas dataframe and a series\n",
        "Documentation on the data set is [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). Name your features dataframe (the independent variables) `df_X` and your class label (the dependent variable) series `s_y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oBRRztuVQNr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "d43d6cd2-7049-4b05-e506-032f6821c340"
      },
      "source": [
        "(df_X, s_y) = load_iris(return_X_y=True, as_frame=True) # 150 rows each\n",
        "df_X"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "0                  5.1               3.5                1.4               0.2\n",
              "1                  4.9               3.0                1.4               0.2\n",
              "2                  4.7               3.2                1.3               0.2\n",
              "3                  4.6               3.1                1.5               0.2\n",
              "4                  5.0               3.6                1.4               0.2\n",
              "..                 ...               ...                ...               ...\n",
              "145                6.7               3.0                5.2               2.3\n",
              "146                6.3               2.5                5.0               1.9\n",
              "147                6.5               3.0                5.2               2.0\n",
              "148                6.2               3.4                5.4               2.3\n",
              "149                5.9               3.0                5.1               1.8\n",
              "\n",
              "[150 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0OUEFGaVQNr"
      },
      "source": [
        "## Q2.2 Partition `df_X` and `s_y` into $5$ partitions of roughly equal size\n",
        "Make 2 dictionaries, with the key of each dictionary the fold number.  The value of the dictionary `dict_k_df_X` is the $k^{th}$ partition of the data, and the value of the dictionary `dict_k_s_y` is the corresponding $k^{th}$ target classification.  Print out the number of rows in each fold.  Check that the number of data points in each partition totals the number of data points in the entire dataset. \n",
        "\n",
        "Note, you can reuse the functions from Section 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FDi6t03VQNs"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "arr_X = df_X.to_numpy()\n",
        "arr_y = s_y.to_numpy()\n",
        "kfolds = KFold(n_splits=5)\n",
        "\n",
        "dict_k_df_X = {}\n",
        "dict_k_s_y = {}\n",
        "df_X_indices = []\n",
        "s_y_indices = []\n",
        "part = -1\n",
        "\n",
        "for train, test in kfolds.split(arr_X):\n",
        "  part += 1\n",
        "  df_X_indices.append(test)\n",
        "\n",
        "part = -1\n",
        "for train, test in kfolds.split(arr_y):\n",
        "  part += 1\n",
        "  s_y_indices.append(test)\n",
        "\n",
        "for idx, lst in enumerate(df_X_indices):\n",
        "  dict_k_df_X[idx] = df_X[lst[0]:(lst[-1]+1)]\n",
        "for idx, lst in enumerate(s_y_indices):\n",
        "  dict_k_s_y[idx] = s_y[lst[0]:(lst[-1]+1)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yiKEKRvdd1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244286e7-e255-448c-e830-1878e9f50126"
      },
      "source": [
        "# input: X db, y Series, k\n",
        "# output: tuple (db_X, s_y) with exclusion applied\n",
        "def df_exclude_k(df_X, s_y, k):\n",
        "    excluded_df = pd.DataFrame(dict_k_df_X[k])\n",
        "    excluded_s = pd.Series(dict_k_s_y[k])\n",
        "\n",
        "    df_y = s_y.to_frame() # series into df\n",
        "    df_new = df_X.merge(excluded_df, how='left', indicator=True)\n",
        "    df_new = df_new[df_new['_merge'] == 'left_only']\n",
        "    df_new = df_new.drop(columns=['_merge'])\n",
        "\n",
        "    start_ind = excluded_s.index[0]\n",
        "    stop_ind = excluded_s.index[-1]\n",
        "\n",
        "    s_new_1 = s_y.iloc[0:start_ind]\n",
        "    s_new_2 = s_y.iloc[stop_ind+1:]\n",
        "    \n",
        "    s_new = s_new_1.combine_first(s_new_2)\n",
        "\n",
        "    if k > 2: # FIXME: length mismatch at k == 3 || 4\n",
        "        s_new = s_new.iloc[:-1]\n",
        "\n",
        "    return df_new, s_new # X, y format\n",
        "\n",
        "df_exclude_k(df_X, s_y, 3)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              " 0                  5.1               3.5                1.4               0.2\n",
              " 1                  4.9               3.0                1.4               0.2\n",
              " 2                  4.7               3.2                1.3               0.2\n",
              " 3                  4.6               3.1                1.5               0.2\n",
              " 4                  5.0               3.6                1.4               0.2\n",
              " ..                 ...               ...                ...               ...\n",
              " 145                6.7               3.0                5.2               2.3\n",
              " 146                6.3               2.5                5.0               1.9\n",
              " 147                6.5               3.0                5.2               2.0\n",
              " 148                6.2               3.4                5.4               2.3\n",
              " 149                5.9               3.0                5.1               1.8\n",
              " \n",
              " [119 rows x 4 columns], 0      0.0\n",
              " 1      0.0\n",
              " 2      0.0\n",
              " 3      0.0\n",
              " 4      0.0\n",
              "       ... \n",
              " 144    2.0\n",
              " 145    2.0\n",
              " 146    2.0\n",
              " 147    2.0\n",
              " 148    2.0\n",
              " Name: target, Length: 119, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3QvbgqsVQNs"
      },
      "source": [
        "## Q2.3 Define a function that calculates accuracy\n",
        "The function should accept two series and compare each element for equality.  The accuracy is the number of equal elements divided by the total number of elements.\n",
        "\n",
        "Test your accuracy function by calling it with the `s_y` loaded from the iris data set and an array of the same length containing all $1$ values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n6tXavdUak4"
      },
      "source": [
        "# create a function that takes dictionary and current fold,\n",
        "# and returns a dictionary with all OTHER folds\n",
        "def merge_folds(dict_X, dict_y, curr_k):\n",
        "    del dict_X[curr_k]\n",
        "    del dict_y[curr_k]\n",
        "    return dict_X, dict_y"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGEZaKCRVQNs"
      },
      "source": [
        "def get_acc( s_1, s_2 ): # assume same Series length\n",
        "  num_equal = 0\n",
        "  total_elements = s_1.size\n",
        "  for e1 in s_1:\n",
        "    for e2 in s_2:\n",
        "      if e2 == e1:\n",
        "        num_equal += 1\n",
        "        break # stop comparing to not count e1 element multiple times\n",
        "\n",
        "  return num_equal/total_elements"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8WcPX7IVQNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699597c7-6738-4001-d2dd-1bcddfd81573"
      },
      "source": [
        "get_acc(s_y,np.ones(len(s_y)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1szoVAVkVQNs"
      },
      "source": [
        "## Q2.4 Using Nested Cross validation, find the best hyperparameter\n",
        "Use the [Decision Tree Classifier](https://scikit-learn.org/stable/modules/tree.html#classification) class to build a decision tree inside of a 5-fold cross validation loop.  The partitions you already created in 2.2 will be the outer loop.  In the inside loop you should use 4-fold cross validation (so you don't have to partition _again_) to find the best value for `min_impurity_decrease`.  Use the Gini Index as your impurity measure. \n",
        "    Calculate the mean accuracy across the 4 folds of your inner loop for all the candidate `min_impurity_decrease` values, and print the value.  Use the array `np.array([0.1,0.25,0.3,0.4])` as the candidates for the best hyperparameter. If there is a tie (two `min_impurity_decrease` values give the same highest accuracy), choose the lowest `min_impurity_decrease` value. \n",
        "\n",
        "For each inner loop, select the best `min_impurity_decrease` and train the outer fold training data on using that value. \n",
        "\n",
        "For each of the 5 executions of the inner loop, your output should look something like this:\n",
        "```\n",
        "Testing 0.10 min impurity decrease\n",
        "\tAverage accuracy over 4 folds is 0.95\n",
        "Testing 0.25 min impurity decrease\n",
        "\tAverage accuracy over 4 folds is 0.86\n",
        "Testing 0.30 min impurity decrease\n",
        "\tAverage accuracy over 4 folds is 0.63\n",
        "Testing 0.40 min impurity decrease\n",
        "\tAverage accuracy over 4 folds is 0.27\n",
        "\n",
        "Best min impurity decrease is 0.1\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taTfwyVhaYKM"
      },
      "source": [
        "# import libraries from SKLearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def run_cross_validation_on_trees(X, y, min_impurity_decrease, cv=4, scoring='accuracy'):\n",
        "    cv_scores_list = []\n",
        "    cv_scores_std = []\n",
        "    cv_scores_mean = []\n",
        "    accuracy_scores = []\n",
        "    #for depth in tree_depths:\n",
        "    tree_model = DecisionTreeClassifier(min_impurity_decrease=pos_min_impurity) # default impurity = GINI\n",
        "    cv_scores = cross_val_score(tree_model, X, y, cv=cv, scoring=scoring)\n",
        "\n",
        "    cv_scores_list.append(cv_scores)\n",
        "    cv_scores_mean.append(cv_scores.mean())\n",
        "    cv_scores_std.append(cv_scores.std())\n",
        "\n",
        "    accuracy_scores.append(tree_model.fit(X, y).score(X, y))\n",
        "    cv_scores_mean = np.array(cv_scores_mean)\n",
        "    cv_scores_std = np.array(cv_scores_std)\n",
        "    accuracy_scores = np.array(accuracy_scores)\n",
        "\n",
        "    return cv_scores_list, cv_scores_mean, accuracy_scores # ORDER\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a7spNAnVQNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca9d5ef-e4d7-4b63-9da7-1dc0b0d6175b"
      },
      "source": [
        "possible_min_impurity_decrease = np.array([0.1,0.25,0.3,0.4]) # the answer is one of these\n",
        "# Outer loop\n",
        "outer_acc = np.array([])\n",
        "for k in dict_k_df_X.keys():\n",
        "\n",
        "    k_test_y = dict_k_s_y[k] # partition 0 y\n",
        "    k_inner_X, k_inner_y = df_exclude_k(df_X, s_y, k) # partition 1-4 X, y\n",
        "\n",
        "    # print(\"k = \", k, \" List, Means, and Accuracies:\")\n",
        "    \n",
        "    # HAVE: outer testing y, raw inner TRAINING data\n",
        "    for pos_min_impurity in possible_min_impurity_decrease:\n",
        "        # print(pos_min_impurity, \": \")\n",
        "        cv_list, cv_means, accuracy_scores = run_cross_validation_on_trees(k_inner_X, k_inner_y, pos_min_impurity) # use defaults\n",
        "        # print(accuracy_scores)\n",
        "        # need to keep track of average accuracy for each of pos_min_imp levels\n",
        "        outer_acc = np.append(outer_acc, accuracy_scores)\n",
        "\n",
        "ind01 = list(range(0, 17, 4))\n",
        "ind025 = list(range(1, 17, 4))\n",
        "ind03 = list(range(2, 17, 4))\n",
        "ind04 = list(range(3, 17, 4))\n",
        "sum01, sum025, sum03, sum04 = 0, 0, 0, 0\n",
        "\n",
        "for idx, avg_acc in enumerate(outer_acc):\n",
        "    if idx in ind01:\n",
        "      sum01 += avg_acc\n",
        "    elif idx in ind025:\n",
        "      sum025 += avg_acc\n",
        "    elif idx in ind03:\n",
        "      sum03 += avg_acc\n",
        "    elif idx in ind04:\n",
        "      sum04 += avg_acc\n",
        "    \n",
        "avg01 = sum01 / 5\n",
        "avg025 = sum025 / 5\n",
        "avg03 = sum03 / 5\n",
        "avg04 = sum04 / 5\n",
        "\n",
        "print('Testing 0.10 min impurity decrease')\n",
        "print('\\tAverage accuracy over 4 folds is {:.2f}'.format(avg01))\n",
        "print('Testing 0.25 min impurity decrease')\n",
        "print('\\tAverage accuracy over 4 folds is {:.2f}'.format(avg025))\n",
        "print('Testing 0.30 min impurity decrease')\n",
        "print('\\tAverage accuracy over 4 folds is {:.2f}'.format(avg03))\n",
        "print('Testing 0.40 min impurity decrease')\n",
        "print('\\tAverage accuracy over 4 folds is {:.2f}'.format(avg04))\n",
        "\n",
        "avgs = {.10: avg01,\n",
        "        .25: avg025,\n",
        "        .30: avg03,\n",
        "        .40: avg04\n",
        "        }\n",
        "best_min_impurity = max(avgs, key=avgs.get)\n",
        "print('\\nBest min impurity decrease is {:.2f}'.format(best_min_impurity))\n",
        "\n",
        "\n",
        "# now use np.mean(sum??) for mean, and determine the minimum of those values\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    # Use best min impurity decrease to train model\n",
        "    \n",
        "    # outer accuracy calculation \n",
        "\n",
        "    #outer_acc = np.append(outer_acc,this_acc) # make sure and calculate this_acc in your loop"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 0.10 min impurity decrease\n",
            "\tAverage accuracy over 4 folds is 0.96\n",
            "Testing 0.25 min impurity decrease\n",
            "\tAverage accuracy over 4 folds is 0.66\n",
            "Testing 0.30 min impurity decrease\n",
            "\tAverage accuracy over 4 folds is 0.56\n",
            "Testing 0.40 min impurity decrease\n",
            "\tAverage accuracy over 4 folds is 0.33\n",
            "\n",
            "Best min impurity decrease is 0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6-dgURkVQNt"
      },
      "source": [
        "## Q2.5 Show the generalized performance of the classifier \n",
        "Show the generalized performance of the classifier by printing the min, max, and mean accuracy of the outer fold test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wed-YLSVQNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be09be44-f792-47f2-d7f9-e6a4545e8d3c"
      },
      "source": [
        "print('Minimum Accuracy of Outer Fold Test Set: ', min(outer_acc))\n",
        "print('Maximum Accuracy of Outer Fold Test Set: ', max(outer_acc))\n",
        "print('Mean Accuracy of Outer Fold Test Set: ', outer_acc.mean())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum Accuracy of Outer Fold Test Set:  0.4166666666666667\n",
            "Maximum Accuracy of Outer Fold Test Set:  0.9833333333333333\n",
            "Mean Accuracy of Outer Fold Test Set:  0.735406162464986\n"
          ]
        }
      ]
    }
  ]
}